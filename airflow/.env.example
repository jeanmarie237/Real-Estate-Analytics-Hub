# ─── Airflow core ─────────────────────────────────────────────────────────────
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY=

AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__WEBSERVER__SECRET_KEY=

# ─── AWS S3 Connection ────────────────────────────────────────────────────────
# Configure in Airflow UI: Admin → Connections → aws_conn
# Or set via environment variable (URI format):
# AIRFLOW_CONN_AWS_CONN=aws://ACCESS_KEY_ID:SECRET_ACCESS_KEY@?region_name=eu-west-3
AIRFLOW_CONN_AWS_CONN=

# ─── Snowflake Connection ─────────────────────────────────────────────────────
# Configure in Airflow UI: Admin → Connections → snowflake_default
# Connection type: Snowflake
# Host    : ZPSXDDX-FD93437.snowflakecomputing.com
# Schema  : DEV_BRONZE
# Login   : <your_user>
# Password: <your_password>
# Extra   : {"warehouse": "DVF_WH", "database": "DVF_DB", "role": "DVF_BI_ROLE"}

# ─── S3 Bucket (used in DAG code) ─────────────────────────────────────────────
DVF_S3_BUCKET=data-platform-project-kubctl-1
DVF_S3_PREFIX=real-raw/

# ─── Copy this file to .env and fill in your values ──────────────────────────
# cp airflow/.env.example airflow/.env
# .env is gitignored — never commit credentials
