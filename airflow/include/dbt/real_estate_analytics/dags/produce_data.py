# # import requests
# # from bs4 import BeautifulSoup
# # from io import BytesIO
# # import zipfile
# # import os

# # # Dossier o√π tu stockes les fichiers extraits
# # os.makedirs("raw", exist_ok=True)

# # # URL de la page des DVF
# # url = "https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/"

# # # R√©cup√©rer le HTML
# # response = requests.get(url)
# # soup = BeautifulSoup(response.content, "html.parser")

# # # Rechercher tous les liens ZIP
# # zip_links = []
# # for a in soup.find_all("a", href=True):
# #     if a["href"].endswith(".zip"):
# #         zip_links.append(a["href"])

# # print(f"{len(zip_links)} fichiers ZIP trouv√©s.")

# # # T√©l√©charger et extraire chaque ZIP
# # for link in zip_links:
# #     # URL compl√®te si besoin
# #     full_url = link
# #     if not link.startswith("http"):
# #         full_url = "https://www.data.gouv.fr" + link
    
# #     print(f"T√©l√©chargement : {full_url}")
# #     r = requests.get(full_url)
    
# #     if r.status_code == 200:
# #         with zipfile.ZipFile(BytesIO(r.content)) as z:
# #             for file_name in z.namelist():
# #                 if file_name.endswith(".zip"):
# #                     print(f"Extraction du fichier : {file_name}")
# #                     z.extract(file_name, path="raw")
# #     else:
# #         print(f"Erreur t√©l√©chargement : {r.status_code}")


# import requests
# import zipfile
# from io import BytesIO
# import os

# RAW_DIR = "raw"
# os.makedirs(RAW_DIR, exist_ok=True)

# DATASET_API = "https://www.data.gouv.fr/api/1/datasets/demandes-de-valeurs-foncieres/"

# response = requests.get(DATASET_API)
# response.raise_for_status()

# dataset = response.json()
# resources = dataset["resources"]

# zip_files = [
#     r for r in resources
#     if r["url"].endswith(".zip") and "valeursfoncieres" in r["url"]
# ]

# print(f"{len(zip_files)} fichiers ZIP DVF d√©tect√©s")

# for r in zip_files:
#     url = r["url"]
#     filename = url.split("/")[-1]

#     print(f"T√©l√©chargement : {filename}")

#     resp = requests.get(url)
#     resp.raise_for_status()

#     with zipfile.ZipFile(BytesIO(resp.content)) as z:
#         z.extractall(RAW_DIR)

# print("‚úÖ Extraction DVF termin√©e")

import requests
import zipfile
from io import BytesIO
import os
import re

RAW_DIR = "raw"
os.makedirs(RAW_DIR, exist_ok=True)

DATASET_API = "https://www.data.gouv.fr/api/1/datasets/demandes-de-valeurs-foncieres/"

response = requests.get(DATASET_API)
response.raise_for_status()

dataset = response.json()
resources = dataset["resources"]

zip_files = [
    r for r in resources
    if r["url"].endswith(".zip") and "valeursfoncieres" in r["url"]
]

print(f"{len(zip_files)} fichiers DVF d√©tect√©s")

for r in zip_files:
    url = r["url"]
    zip_name = url.split("/")[-1]
    year_match = re.search(r"20\d{2}", zip_name)

    # nom du fichier txt attendu apr√®s extraction
    extracted_files = [
        f for f in os.listdir(RAW_DIR)
        if year_match and year_match.group() in f
    ]

    if extracted_files:
        print(f"‚è≠Ô∏è  {zip_name} d√©j√† pr√©sent ‚Üí SKIP")
        continue

    print(f"‚¨áÔ∏è  T√©l√©chargement : {zip_name}")
    resp = requests.get(url)
    resp.raise_for_status()

    with zipfile.ZipFile(BytesIO(resp.content)) as z:
        z.extractall(RAW_DIR)

    print(f"‚úÖ {zip_name} extrait")

print("üéØ Ingestion DVF termin√©e")

